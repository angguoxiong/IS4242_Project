{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15adead0",
   "metadata": {},
   "source": [
    "# List of Packages required for Scrapping and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09f10651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scrapy.selector import Selector\n",
    "from selenium import webdriver \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c41dee",
   "metadata": {},
   "source": [
    "## Instantiate the data required in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d8ff9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User List can be put into a list but we are extracting 1 by 1 for better and efficient results\n",
    "users_list = ['ur3793011']\n",
    "#error - ur15148330, ur104603847\n",
    "user_id = []\n",
    "title_list = []\n",
    "description_list = []\n",
    "img_list = []\n",
    "year_list = []\n",
    "director_list = []\n",
    "star_list = []\n",
    "duration_list = []\n",
    "advisory_list = []\n",
    "genre_list = []\n",
    "vote_list = []\n",
    "movie_rating_list = []\n",
    "user_rating_list = []\n",
    "img_file_list = []\n",
    "error_msg = []\n",
    "\n",
    "record_id = 1\n",
    "record_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78cc80",
   "metadata": {},
   "source": [
    "**The code that runs through IMDB ratings website for the particular users and extract important information required for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f75eee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for User 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 33.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 95.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 94.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 86.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 89.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 91.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 95.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 80.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 95.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 97.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 89.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 105.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 99.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 91.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 91.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 88.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 98.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 96.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 87.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 92.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to browse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for user in range(len(users_list)):\n",
    "    print(\"Scraping for User {}\".format(user+1))\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    url = 'https://www.imdb.com/user/{}/ratings'.format(users_list[user])\n",
    "    time.sleep(1)\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    sel = Selector(text = driver.page_source)\n",
    "    num_of_ratings = sel.css(\".lister-list-length span::text\").extract_first().replace(',','').split(' ')[0]\n",
    "    rating_pages = int(int(num_of_ratings)/100) + 1  \n",
    "    user_id += [users_list[user] for i in range(int(num_of_ratings))]\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    count = 0\n",
    "    for x in range(rating_pages):\n",
    "        sel = Selector(text = driver.page_source)\n",
    "        reviews = driver.find_elements(By.CSS_SELECTOR, 'div.lister-item.mode-detail')\n",
    "        \n",
    "        # filter out the images that have a title attribute with the value \"list image\"\n",
    "        ##Get img url\n",
    "        image_tags = soup.find_all('img', {'class': 'loadlate', 'title': lambda value: value is None or value != 'list image'})\n",
    "        for img_tag in image_tags:\n",
    "            # Get the URL of the movie poster image\n",
    "            img_url = img_tag['loadlate']\n",
    "            img_list.append(img_url)\n",
    "        \n",
    "        for review in tqdm(reviews):\n",
    "            \n",
    "            try:\n",
    "                sel2 = Selector(text = review.get_attribute('innerHTML'))\n",
    "                \n",
    "                ## Get movie title\n",
    "                try:\n",
    "                    title = sel2.css('.lister-item-content a::text').extract_first().strip()\n",
    "                    episode = sel2.css('.lister-item-content a::text').getall()[1].strip()\n",
    "                    if episode != \"\":\n",
    "                        title += (\" - \" + episode)\n",
    "                except:\n",
    "                    title= np.NaN\n",
    "                    \n",
    "                ## Get movie description\n",
    "                try:\n",
    "                    advisory = sel2.css('.certificate::text').extract_first()\n",
    "                    duration = sel2.css('.runtime::text').extract_first()\n",
    "                    description = None\n",
    "                    if advisory == None and duration == None:\n",
    "                        description = sel2.css('p::text').getall()[3].strip()\n",
    "                    elif advisory == None or duration == None:\n",
    "                        description = sel2.css('p::text').getall()[5].strip()\n",
    "                    else:\n",
    "                        description = sel2.css('p::text').getall()[7].strip()\n",
    "                except:\n",
    "                    description = np.NaN\n",
    "                ## Get movie year\n",
    "                try:\n",
    "                    year = sel2.css('.lister-item-year.text-muted.unbold::text').extract_first().strip().replace('(','').replace(')','')\n",
    "                    year = re.sub(r'[a-zA-Z\\s]+', '', year)\n",
    "                except:\n",
    "                    year = np.NaN\n",
    "                ## Get directors and staff\n",
    "                try:\n",
    "                    staff = sel2.css('.text-muted a::text').getall()\n",
    "                    text = sel2.css('.text-muted.text-small::text').getall()\n",
    "                    text2 = [x.strip() for x in text]\n",
    "                    commas = text2.count(',')\n",
    "                    stars_index = text2.index(\"Stars:\")\n",
    "                    count = 0\n",
    "                    for i in range(stars_index, len(text2)-1):\n",
    "                        if text2[i] == ',':\n",
    "                            count+=1\n",
    "                    stars = staff[-(count+1):]\n",
    "                    # if directors are recorded\n",
    "                    if \"Director:\" in text2 or \"Directors:\" in text2:\n",
    "                        directors = staff[:(commas-count)+1]\n",
    "                    else:\n",
    "                        directors = \"\"\n",
    "                except:\n",
    "                    stars = np.NaN\n",
    "                    directors = np.NaN\n",
    "                ## Get movie duration\n",
    "                try:\n",
    "                    duration = duration.strip()\n",
    "                except:\n",
    "                    duration = np.NaN\n",
    "                ## Get viewer advisory\n",
    "                try:\n",
    "                    advisory = advisory.strip()\n",
    "                except:\n",
    "                    advisory = np.NaN\n",
    "                ## Get Genre\n",
    "                try:\n",
    "                    genre = sel2.css('.genre::text').extract_first().strip()\n",
    "                except:\n",
    "                    genre = np.NaN\n",
    "                ## Get votes\n",
    "                try:\n",
    "                    votes = sel2.css('.text-muted.text-small span::text').getall()[-1]\n",
    "                    votes = int(votes.replace(',','').split(' ')[0])\n",
    "                except:\n",
    "                    votes = np.NaN\n",
    "                ## Get movie rating\n",
    "                try:\n",
    "                    movie_rating = sel2.css('.ipl-rating-star__rating::text').getall()[0]\n",
    "                    movie_rating = float(movie_rating.replace(',','').split(' ')[0])\n",
    "                except:\n",
    "                    movie_rating = np.NaN\n",
    "                ## Get user rating\n",
    "                try:\n",
    "                    user_rating = sel2.css('.ipl-rating-star__rating::text').getall()[1]\n",
    "                    user_rating = int(user_rating.replace(',','').split(' ')[0])\n",
    "                except:\n",
    "                    user_rating = np.NaN\n",
    "                    \n",
    "                try:\n",
    "                    title_rename = re.sub(r'[^\\w\\s!-]|[.!?]', '', title)\n",
    "                    img_name = f\"{user_id[0]}_{record_id}.jpg\"\n",
    "                except:\n",
    "                    img_name = np.NaN\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                title_list.append(title)\n",
    "                description_list.append(description)\n",
    "                year_list.append(year)\n",
    "                director_list.append(directors)\n",
    "                star_list.append(stars)\n",
    "                duration_list.append(duration)\n",
    "                advisory_list.append(advisory)\n",
    "                genre_list.append(genre)\n",
    "                vote_list.append(votes)\n",
    "                movie_rating_list.append(movie_rating)\n",
    "                user_rating_list.append(user_rating)\n",
    "                img_file_list.append(img_name)\n",
    "                record_list.append(record_id)\n",
    "                record_id  += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg.append(e)\n",
    "        \n",
    "        try:\n",
    "            next_page_url = sel.css(\"a.flat-button.lister-page-next.next-page::attr(href)\").extract_first()\n",
    "            full_next_page_url = \"https://www.imdb.com\" + next_page_url\n",
    "            driver.get(full_next_page_url)\n",
    "            response = requests.get(full_next_page_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        except:\n",
    "            print(\"No more pages to browse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef069371",
   "metadata": {},
   "source": [
    "**Storing the data into a dataframe and exporting it to a csv file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bbb4ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all data in dataframe\n",
    "rating_df = pd.DataFrame({\n",
    "     \"UserID\": user_id,\n",
    "     \"record_id\": record_list,\n",
    "     \"Title\":title_list,\n",
    "     \"Img_Path\": img_list,\n",
    "     \"Img_File_Name\": img_file_list,\n",
    "     \"Year\":year_list,\n",
    "     \"Description\":description_list,\n",
    "     \"Directors\":director_list,\n",
    "     \"Stars\": star_list,\n",
    "     \"Viewer_Advisory\": advisory_list,\n",
    "     \"Duration\": duration_list,\n",
    "     \"Genre\": genre_list,\n",
    "     \"Votes\": vote_list, \n",
    "     \"Movie_Rating\": movie_rating_list,\n",
    "     \"User_Rating\": user_rating_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6256457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset\n",
    "rating_df.to_csv(path_or_buf = f\"{user_id[0]}_ratings.csv\"\n",
    "                          , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8ef16",
   "metadata": {},
   "source": [
    "## Download the Images to the folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ac2bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading image from https://m.media-amazon.com/images/S/sash/i-t32yvKixg10fG.png: cannot write mode P as JPEG\n"
     ]
    }
   ],
   "source": [
    "##image downloading\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "        img_save_path = save_path\n",
    "        img.save(img_save_path, format=\"JPEG\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image from {url}: {e}\")\n",
    "\n",
    "img_folder = f\"{user_id[0]}_downloaded_images\"\n",
    "os.makedirs(img_folder, exist_ok=True)\n",
    "\n",
    "for idx, row in rating_df.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    title = row['Title']\n",
    "    img_url = row['Img_Path']\n",
    "    record_id = row[\"record_id\"]\n",
    "   \n",
    "    img_filename = f\"{user_id}_{record_id}.jpg\"\n",
    "    img_save_path = os.path.join(img_folder, img_filename)\n",
    "\n",
    "    download_image(img_url, img_save_path)\n",
    "    \n",
    "    #See image in python\n",
    "    #img = Image.open(img_save_path)\n",
    "    #plt.imshow(img)\n",
    "    #plt.savefig(img_save_path)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fdcc8",
   "metadata": {},
   "source": [
    "## Image Analysis using KMeans Clustering (3 main colours and Brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2285af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import pandas as pd\n",
    "import webcolors\n",
    "\n",
    "def get_main_colors(image, k=3):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.reshape(image.shape[0] * image.shape[1], 3)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(image)\n",
    "    \n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "def get_brightness(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    r, g, b = np.split(image, 3, axis=-1)\n",
    "    brightness = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    return np.mean(brightness)\n",
    "\n",
    "def get_saturation(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, saturation, _ = cv2.split(hsv_image)\n",
    "    return np.mean(saturation) / 255.0\n",
    "\n",
    "def get_hue(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hue, _, _ = cv2.split(hsv_image)\n",
    "    return np.mean(hue) / 180.0\n",
    "\n",
    "def get_texture(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, th = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    edges = cv2.Canny(th, 100, 200)\n",
    "    return np.mean(edges) / 255.0\n",
    "\n",
    "def get_entropy(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "    hist /= np.sum(hist)\n",
    "    hist = hist[np.nonzero(hist)]\n",
    "    entropy = -np.sum(hist * np.log2(hist))\n",
    "    return entropy\n",
    "\n",
    "def get_noise(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    mean = np.mean(gray_image)\n",
    "    variance = np.mean(np.square(gray_image - mean))\n",
    "    return variance\n",
    "\n",
    "def get_colorfulness(image):\n",
    "    (B, G, R) = cv2.split(image.astype(\"float\"))\n",
    "    rg = np.absolute(R - G)\n",
    "    yb = np.absolute(0.5 * (R + G) - B)\n",
    "    rb_mean = np.mean(rg)\n",
    "    yb_mean = np.mean(yb)\n",
    "    colorfulness = np.sqrt(np.square(rb_mean) + np.square(yb_mean)) + 0.3 * np.std(rg + yb)\n",
    "    return colorfulness / 220.0\n",
    "\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image '{image_path}'\")\n",
    "        return None, None, None, None\n",
    "    main_colors = get_main_colors(image)\n",
    "    color1 = main_colors[0].tolist()\n",
    "    color2 = main_colors[1].tolist()\n",
    "    color3 = main_colors[2].tolist()\n",
    "    brightness = get_brightness(image)\n",
    "    saturation = get_saturation(image)\n",
    "    hue = get_hue(image)\n",
    "    texture = get_texture(image)\n",
    "    entropy = get_entropy(image)\n",
    "    noise = get_noise(image)\n",
    "    colorfulness = get_colorfulness(image)\n",
    "    \n",
    "    \n",
    "    # Calculate contrast\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    #brightness = np.mean(gray_image) / 255.0\n",
    "    contrast = (np.max(gray_image) - np.min(gray_image)) / (np.max(gray_image) + np.min(gray_image))\n",
    "    contrast = 0 if np.isinf(contrast) else contrast\n",
    "   \n",
    "    return color1, color2, color3, brightness, contrast, saturation, hue, texture, entropy, noise, colorfulness\n",
    "\n",
    "folder_path = f'{user_id}_downloaded_images'\n",
    "image_files = os.listdir(folder_path)\n",
    "\n",
    "image_names = []\n",
    "color1_list = []\n",
    "color2_list = []\n",
    "color3_list = []\n",
    "brightness_list = []\n",
    "contrast_list = []\n",
    "saturation_list = []\n",
    "hue_list = []\n",
    "texture_list = []\n",
    "entropy_list = []\n",
    "noise_list = []\n",
    "colorfulness_list = []\n",
    "\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    color1, color2, color3, brightness, contrast, saturation, hue, texture, entropy, noise, colorfulness = analyze_image(image_path)\n",
    "    \n",
    "    if color1 is not None and brightness is not None:\n",
    "        image_names.append(image_file)\n",
    "        color1_list.append(color1)\n",
    "        color2_list.append(color2)\n",
    "        color3_list.append(color3)\n",
    "        brightness_list.append(brightness)\n",
    "        contrast_list.append(contrast)\n",
    "        saturation_list.append(saturation)\n",
    "        hue_list.append(hue)\n",
    "        texture_list.append(texture)\n",
    "        entropy_list.append(entropy)\n",
    "        noise_list.append(noise)\n",
    "        colorfulness_list.append(colorfulness)\n",
    "    else:\n",
    "        print(f\"Skipping '{image_file}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eb0b1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Img_File_Name\": image_names,\n",
    "    \"Color1\": color1_list,\n",
    "    \"Color2\": color2_list,\n",
    "    \"Color3\": color3_list,\n",
    "    \"Brightness\": brightness_list,\n",
    "    \"Contrast\": contrast_list,\n",
    "    \"Saturation\": saturation_list,\n",
    "    \"Hue\": hue_list,\n",
    "    \"Texture\": texture_list,\n",
    "    \"Entropy\": entropy_list,\n",
    "    \"Noise\": noise_list,\n",
    "    \"Colorfulness\": colorfulness_list\n",
    "})\n",
    "\n",
    "df.to_csv(f\"{user_id}_image_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547b772",
   "metadata": {},
   "source": [
    "### Merge the data together and form into one final csv file for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b605d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(f'{user_id}_ratings.csv')\n",
    "df_colors = pd.read_csv(f\"{user_id}_image_analysis.csv\")\n",
    "\n",
    "merged_df = pd.merge(df_ratings, df_colors, on='Img_File_Name')\n",
    "\n",
    "colors_columns = ['Color1', 'Color2', 'Color3', 'Brightness', 'Contrast', 'Saturation', 'Hue', 'Texture', 'Entropy', 'Noise', 'Colorfulness']\n",
    "ratings_columns = [\"UserID\",\n",
    "     \"Title\",\n",
    "     \"Img_Path\",\n",
    "     \"Img_File_Name\",\n",
    "     \"Year\",\n",
    "     \"Description\",\n",
    "     \"Directors\",\n",
    "     \"Stars\",\n",
    "     \"Viewer_Advisory\",\n",
    "     \"Duration\",\n",
    "     \"Genre\",\n",
    "     \"Votes\",\n",
    "     \"Movie_Rating\",\n",
    "     \"User_Rating\"]\n",
    "\n",
    "final_merged = merged_df[ratings_columns + colors_columns]\n",
    "\n",
    "final_merged.to_csv(f'{user_id}_img_ratings.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf6755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
